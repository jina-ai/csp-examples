{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and inference Reader LM v2 with Google Cloud Marketplace Virtual machine\n",
    "\n",
    "This notebook demonstrates how to deploy a Reader LM V2 model-powered [Google Cloud Marketplace Virtual machine](https://console.cloud.google.com/marketplace/product/jinaai-public/reader-lm-v2) and perform inference with the application within the virtual machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the managed application\n",
    "\n",
    "To deploy the virtual machine, start by consulting the [official deployment guide](https://cloud.google.com/marketplace/docs/deploy-through-CLI#use-snippet). This document provides comprehensive steps for the deployment process.\n",
    "\n",
    "It's worth mentioning that in the Basics tab of the deployment setup, you will need to provide several details about the VM. \n",
    "\n",
    "You can customize the VM size used, and for certain sizes, you might need to adjust the allowed quota to ensure access. It is recommended to select \"GPUs\" tab and use the [NVIDIA_L4](https://cloud.google.com/compute/docs/gpus?_gl=1*l4wato*_ga*MTI3NjQzMzcwNy4xNzI3NTg1MDYx*_ga_WH2QY8WWF5*MTczMDA4MDMzMC42NC4xLjE3MzAwODQ4NTQuMzcuMC4w#general_comparison_chart). This VM features up to 1 NVIDIA L4 GPU with 24 GB of memory. The boot disk size should be minimum 30 GB.\n",
    "\n",
    "For the other tabs, you can leave most settings as default or adjust them to fit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deploy_reader_lm_v2.png\" width=\"50%\" height=\"50%\" style=\"max-width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment of the VM is complete, proceed to the resource group created for your deployment to verify the resources that have been established. \n",
    "\n",
    "Within the resource group, you'll find the public IP through which you can access your application within the VM.\n",
    "\n",
    "Please note that the application within the VM will be unavailable immediately after deployment due to necessary model loading process **We recommend waiting at least 7 minutes before using the application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference with the managed application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct the prompt using `create_prompt`, where you can specify the desired return format. This will improve the results according to specific scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text: str, return_type: str, instruction: str = None, schema: str = None):\n",
    "    \"\"\"\n",
    "    Creates a prompt based on the specified return type (either 'json' or 'markdown').\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The HTML content that needs to be converted into the desired format.\n",
    "    - return_type (str): The desired return format. It must be either \"json\" or \"markdown\".\n",
    "    - instruction (str): The instruction to be included in the prompt. If not provided, a default instruction is used.\n",
    "    - schema (str): The JSON schema for structuring the output (used only for 'json' return_type). If empty, no schema is included.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if return_type not in [\"json\", \"markdown\"]:\n",
    "        raise ValueError(\"Invalid return_type. Must be 'json' or 'markdown'.\")\n",
    "    \n",
    "    if return_type == \"json\":\n",
    "        if not instruction:\n",
    "            instruction = \"Extract the main content from the given HTML and convert it in a structured JSON format.\"\n",
    "\n",
    "        if schema:\n",
    "            prompt = f\"{instruction}\\n```html\\n{text}\\n```\\nThe JSON schema is as follows:```json{schema}```\" \n",
    "        else:\n",
    "            prompt = f\"{instruction}\\n```html\\n{text}\\n```\"\n",
    "    \n",
    "    elif return_type == \"markdown\":\n",
    "        if not instruction:\n",
    "            instruction = \"Extract the main content from the given HTML and convert it to Markdown format.\"\n",
    "        prompt = f\"{instruction}\\n```html\\n{text}\\n```\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Phone Book</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Phone Book</h1>\n",
    "    <div class=\"contact\">\n",
    "        <h2>John Doe</h2>\n",
    "        <p>Email: <a href=\"mailto:john.doe@example.com\">john.doe@example.com</a></p>\n",
    "    </div>\n",
    "    <div class=\"contact\">\n",
    "        <h2>Jane Smith</h2>\n",
    "        <p>Email: <a href=\"mailto:jane.smith@example.com\">jane.smith@example.com</a></p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "prompt = create_prompt(return_type=\"markdown\", text=html)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the invoke function to execute inference, shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def invoke_endpoint():\n",
    "    url = \"http://<Insert here your public IP address>/invocations\"  # With above example, it's \"http://20.84.48.180/invocations\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    json_data = {\n",
    "        \"model\": \"ReaderLM-v2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False  # Whether to stream back partial progress.\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(json_data))\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "invoke_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
