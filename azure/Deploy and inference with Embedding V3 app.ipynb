{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and inference Jina Embeddings V3 with Azure app\n",
    "\n",
    "This notebook demonstrates how to deploy a Jina Embeddings V3 model-powered [Azure managed application](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v3?tab=Overview) and perform inference with this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the managed application\n",
    "\n",
    "To deploy your Azure managed application, start by consulting the [official deployment guide](https://learn.microsoft.com/en-us/azure/azure-resource-manager/managed-applications/deploy-marketplace-app-quickstart). This document provides comprehensive steps for the deployment process.\n",
    "\n",
    "It's worth mentioning that in the Basics tab of the deployment setup, you will need to provide several details about your deployment. \n",
    "\n",
    "You can customize the VM used, and for certain types, you might need to adjust the allowed quota to ensure access. It is recommended to use the [Standard_NC4as_T4_v3](https://learn.microsoft.com/en-us/azure/virtual-machines/nct4-v3-series) VM. This VM features up to 1 NVIDIA T4 GPU with 16 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deploy_embedding_v3_app.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment of the managed application is complete, proceed to the resource group created for your deployment (for instance, `mrg-v3embed-20240919121306` as referenced in the provided screenshot) to verify the resources that have been established. \n",
    "\n",
    "Within this resource group, look for the `jina-inference-vm`. Here, you'll find the DNS Name through which you can access your application. In this example, the application is accessible via `testv2offer.eastus.cloudapp.azure.com`.\n",
    "\n",
    "Please note that the application will be unavailable immediately after deployment due to necessary post-deployment tasks such as driver installation, dependency setup, and system reboot. **We recommend waiting at least 15 minutes before using the application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference with the managed application\n",
    "\n",
    "The Python example below demonstrates how to perform real-time inference using the DNS of the deployed Jina Embeddings V3 model-powered managed application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def invoke_endpoint():\n",
    "    url = \"http://<Insert here your DNS prefix>.<Insert here your region>.cloudapp.azure.com:8080/encode\"  # With above example, it's \"http://testv2offer.eastus.cloudapp.azure.com:8080/encode\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    json_data = {\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"text\": \"How is the weather today?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"What is the weather like today?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"What's the color of an orange?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"What are the hours?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Are you open on weekends?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Are you available on holidays?\"\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"task\": \"text-matching\",  # Select the downstream task for which the embeddings will be used. The model will return the optimized embeddings for that task.\n",
    "            \"late_chunking\": True,  # Apply the late chunking technique to leverage the model's long-context capabilities for generating contextual chunk embeddings.\n",
    "            \"dimensions\": 512  # Output dimensions. Smaller dimensions are easier to store and retrieve, with minimal performance impact thanks to MRL\n",
    "        } \n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(json_data))\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "invoke_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
